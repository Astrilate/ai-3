# 人工智能cv第三轮学习笔记  
  
## 1. 目标检测的相关知识  
  
### 1.1 核心内容  
  
- 目标检测是在图像或视频中识别和定位多个物体实例的任务。  
- 每个检测结果通常包括物体的类别标签和位置信息，用矩形边界框表示。  
  
### 1.2 目标检测的评估指标  
  
- **Intersection over Union (IoU)**：IoU是最常用的目标检测指标之一，它衡量了模型检测框与真实框之间的重叠程度。通常，IoU阈值（如0.5）用于确定检测是否正确。  
- **mAP (mean Average Precision)**：mAP是所有类别的AP的平均值，用于综合评估模型性能。  
  - **Average Precision (AP)**: AP是另一个常用的指标， 它测量模型在不同IoU阈值下的精度。通过计算不同IoU阈值下的Precision-Recall曲线，再计算曲线下的面积  
  - **PR曲线**: Precision-Recall曲线  
  - **Precision**: TP / (TP + FP)  
   - **Recall**: TP / (TP + FN)  
   - **TP**: IoU>0.5的检测框数量  
  - **FP**: IoU<=0.5的检测框，或者多余检测框的数量  
- 一般来说mAP针对整个数据集而言的；AP针对数据集中某一个类别而言的；而percision和recall针对单张图片某一类别的。  
  
### 1.3 常见的目标检测方法  
  
#### 1.3.1 两阶段检测器（如R-CNN系列）  
  
主要思路是先通过启发式方法（selective search）或者CNN网络产生一系列稀疏的候选框，然后对这些候选框进行分类与回归。  
- **R-CNN** (Region-based Convolutional Neural Network)：通过生成候选区域，然后使用CNN对这些区域进行分类和边界框回归，但是速度相对较慢。  
- **Faster R-CNN**：Faster R-CNN引入了区域提议网络（RPN），来生成候选区域。它同样是CNN来提取特征并进行分类和回归。不过Faster R-CNN在准确性和速度上取得了平衡。  
  
#### 1.3.2 单阶段检测器  
  
主要思路是均匀地在图片的不同位置进行密集抽样，抽样时可以采用不同尺度和长宽比，然后利用CNN提取特征后直接进行分类与回归。  
- **YOLO** (You Only Look Once)： YOLO是一种实时目标检测方法，通过单个前向传播来进行检测。它将**图像分成网格**，每个网格预测多个边界框和类别。YOLO最为突出的特点是**速度快**，准确率就比较一般。  
- **SSD** (Single Shot MultiBox Detector) ：SSD和YOLO类似，都是用CNN网络来进行目标检测，也**都采用了多尺度特征图**的检测策略：通过在不同分辨率的图像上检测，来更有效地捕捉不同大小的目标。而SSD采用卷积直接做检测，Yolo在全连接层之后做检测，这里有一些不同。但是SSD相比于YOLO对于**小目标的检测更准确**，而速度就相比YOLO慢。  
  
## 2. Transformer机制的相关知识  
  
### 2.1 核心内容  
  
- Transformer最初被提出用于自然语言处理（NLP）任务，但后来被成功地应用于计算机视觉和其他领域。**它的关键是自注意力机制，它允许模型在不同位置的输入之间建立关联**。在计算机视觉领域中也产生了很多变体，如VIT，DETR，swin transformer等。  
- 在各种各样的任务中，注意力机制已经成为各种引人注目的序列模型和转换模型中的不可或缺的组成部分，它允许对依赖关系建模，而不需要考虑它们在输入或输出序列中的距离。 它只基于单独的attention机制，完全避免使用循环和卷积，且**在处理长序列的问题上更有优势**。  
<img src=https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20va2t3ZWlzaGUvaW1hZ2VzL3Jhdy9tYXN0ZXIvTUwvMjAxOS05LTI1XzIzLTI1LTE0LnBuZw width=500 height=600  />  
  
### 2.2 自注意力机制  
  
自注意力机制是Transformer的核心组成部分，它允许模型在输入序列的不同位置之间分配不同的权重，从而捕获**全局依赖性**。  
  
#### 2.2.1 注意力权重的计算  
  
- 自注意力机制使用三个权重矩阵：**查询（Query）、键（Key）和值（Value）** 来计算注意力权重。  
- 查询矩阵决定了我们关注哪些位置，键矩阵决定了哪些位置与查询更相关，值矩阵包含了要合并的信息。  
- 注意力权重(Scaled Dot-Product Attention)计算公式为:  
  
$$  
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \cdot V  
$$  
  
#### 2.2.2 多头自注意力  
  
单个 attention 所能注意到的信息是有限的，所以通过 multi-head attention 克服这一点。  
- 为了增强模型的表达能力，通常会使用多个自注意力头。多头自注意力计算多个不同的注意力权重，然后将它们合并以获得最终表示。  
- 学习若干组不同的线性映射  W（权值不共享），将 query、key、value 分别投影到  dq、dk、dv  维的空间里（等价于一维卷积），每一组称为一个 head。再对投影后的结果，并行计算 attention，得到  dv  维的值。之后拼接并经过线性映射  W0，得到最终的 multi-head attention。  
- heads过多过少都会损失信息  
<img src=https://sighsmile.github.io/assets/attention.png width=500 height=300 />  
- 以下为多头自注意力实现的一部分代码示例：
	```python
	class Attention(nn.Module):  
	    def __init__(self, dim, num_heads, dim_head=32, dropout=0.3):  
	        super().__init__()  
	        inner_dim = dim_head * num_heads  
	        project_out = not (num_heads == 1 and dim_head == dim)  
	        self.heads = num_heads  
	        self.scale = dim_head ** -0.5  
			self.norm = nn.LayerNorm(dim)  
	        self.attend = nn.Softmax(dim=-1)  
	        self.dropout = nn.Dropout(dropout)  
	        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)  
	        self.to_out = nn.Sequential(  
	            nn.Linear(inner_dim, dim),  
	            nn.Dropout(dropout)  
	        ) if project_out else nn.Identity()  
	  
	    def forward(self, x):  
	        x = self.norm(x)  
	        qkv = self.to_qkv(x).chunk(3, dim=-1)  
	        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', 		    h=self.heads), qkv)  
	        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale  
	        attn = self.attend(dots)  
	        attn = self.dropout(attn)  
	        out = torch.matmul(attn, v)  
	        out = rearrange(out, 'b h n d -> b n (h d)')  
	        return self.to_out(out)
	```
  
### 2.3 编码器和解码器  
  
Transformer模型通常包括编码器和解码器，每个部分都由多层堆叠的自注意力层和前馈神经网络层组成。  
  
#### 2.3.1 编码器  
  
- 编码器用于处理输入序列，例如图像的像素或自然语言句子。它将**输入序列转换为高维表示**。  
- 编码器由N=6个相同层的堆叠组成。每一层都有两个子层。第一种是多头自注意机制，第二种是简单的位置全连接前馈网络。在两个子层的每一个周围采用残差连接，随后是层标准化。  
  
#### 2.3.2 解码器  
  
- 解码器利用高维的信息，**生成输出序列**，如机器翻译的目标语言句子。  
- 解码器也由N=6个相同层的堆栈组成。除了每个编码层中的两个子层之外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意。与编码器类似，在每个子层周围使用残差连接，然后进行层标准化。它还包括额外的遮蔽（mask）层，以防止关注到后续位置(利用掩码以防止位置参与后续的位置运算)。这种掩码与输出嵌入偏移一个位置的事实相结合，确保了对位置i的预测只能依赖于小于i的位置处的已知输出。  
  
### 2.4 位置编码  
  
- 由于Transformer没有明确的位置信息，位置编码被引入来提供输入序列中各个位置的位置信息。位置编码与嵌入具有相同的维度dmodel，因此这两者可以相加。  
- 常见的位置编码方法包括基于正弦和余弦函数的编码。通过正弦和余弦函数的组合，以一种**逐渐减小的频率为不同位置和维度计算位置编码值**。这样，每个位置和维度的位置编码都是唯一的，以便模型能够识别不同位置的标记。公式如下：  
  
$$  
PE_{\left(\text{pos},2i\right)} = \sin\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right) ,  
PE_{\left(\text{pos},2i+1\right)} = \cos\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)  
$$  
  
### 2.5 总结  
  
- Transformer模型在处理**长距离依赖性和全局关联性**方面表现出色。  
- 它易于并行化，使其在大规模数据上训练时具有优势。  
- Transformer需要大量的参数和计算资源，且训练时间长，因此在较小的数据集上可能不适用。 - 对于一维序列数据，如音频信号，传统的RNN和CNN仍然有优势。  
  
## 3. ViT网络的相关知识  
  
### 3.1 核心内容  
  
- Transformer模型在NLP领域研究中显示出主导趋势，这是因为他们有很强的能力通过自注意机制来模拟长期依赖。而ViT主要的思想是将NLP中的transformer机制引入到图像处理中。  
- 在其原论文中提出的核心结论是：当拥有**足够多的数据**进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少**归纳偏置（inductive bias）** 的限制，可以在下游任务中获得较好的迁移效果但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差，因为Transformer和CNN相比缺少归纳偏置，即一种先验知识，提前做好的假设。  
<img src=https://zyc.ai/transformer/vision_transformer/model_scheme.svg  width=600 height=300/>  
  
### 3.2 图像块的划分patch embedding  
  
- Transformer的输入应为序列数据，格式为[batchsize, sentence_len, dim], 而最常用的图像数据格式为[batchsize, channels, height, width], 因此，要将Transformer应用于计算机视觉领域，首先得进行数据转换，即将**图像数据转换为序列数据**。  
- 例如3x224x224的图像，若将图像分为固定大小16x16的patch，则每张图就有14x14+1=197个这样的patch(额外的一个是特殊字符class token，在每个序列的开头)，每个patch维度是3x16x16=768，最后输入张量的形状则为[batchsize, 197, 768]。这其中**将图像碎片进行了扁平化处理**，后续步骤中通过可训练的线性投影映**射到高维进行训练**，再投影回原来的维度进行输出。  
- 以下为代码示例：
	```python
	self.patch_dim = 3 * patch_size * patch_size  
	self.patch_embedding = nn.Sequential(  
	    # 将输入的图像块从二维张量重新排列成一维的块  
	  Rearrange("b c (h p1) (w p2) -> b (h w) (p1 p2 c)", p1=patch_size, p2=patch_size),  
	    nn.LayerNorm(self.patch_dim),  
	    nn.Linear(self.patch_dim, dim),  # 线性变换投影到高维度，从而生成嵌入向量  
	  nn.LayerNorm(dim),  
	)
	```

### 3.3 位置编码的嵌入positional embedding  
  
- 位置编码可以理解为一张表，表一共有N行，N的大小和输入序列长度相同（197），每一行代表一个向量，向量的维度和输入序列的维度相同（768）。通过sum加入位置编码信息之后，输入张量的维度依然是197x768。  
- 部分vit网络的位置嵌入沿用传统transformer的位置嵌入方式，使用正弦余弦函数的方式进行嵌入，而也有部分vit网络使用可学习的位置编码，把参数添加到可训练的参数中一起训练优化，如下方代码。  
- 下方为正弦余弦位置嵌入的代码示例：
	```python
	def position_embedding(h, w, dim, temperature: int = 10000, dtype=torch.float32):  
	    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing="ij")  
	    omega = torch.arange(dim // 4) / (dim // 4 - 1)  
	    omega = 1.0 / (temperature ** omega)  
	    y = y.flatten()[:, None] * omega[None, :]  
	    x = x.flatten()[:, None] * omega[None, :]  
	    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)  
	    return pe.type(dtype)
	    
	self.position_embedding = position_embedding(h=image_size // patch_size, w=image_size // patch_size, dim=dim)
	```
- 下方为可学习位置编码嵌入的代码示例：
	```python  
	self.positions = nn.Parameter(torch.randn((image_size // patch_size) **2 + 1, embedding_size))  
	```  
- 位置编码包括一维和二维的编码方式，一维的编码方式为把每个patch按顺序从0到197编码，二维的编码方式为同时考虑x轴和y轴的信息，类似11,12这种，而vit网络使用的是一维的编码方式，实际上实验表明一维和二维的编码方式对于结果没有太大的区别，至于原因可能是因为位置编码作用在图像块上而不是像素上，图像块之间的位置关系很容易理解。  
  
### 3.4 类别令牌的使用（class token）  
  
- 在3.3的例子中，假设已经把输入图像分割为14x14个16x16x3的图像块后，输入序列为196，由于vit使用transformer的encoder部分作为主体网络，而196的序列的输出也为196个编码向量，最终的分类头需要使用一个向量。所以vit网络就引入了class token作为类别令牌，和196个输入序列一起输入transformer网络中，而**最终分类头进行分类的时候只需要对这一个类别令牌的向量进行分类预测即可**。  
- vit使用上述的类别令牌进行分类预测，原因可能包括以下几点:  
  - 类别序列随机初始化，并随着网络的训练不断更新，它能够编码**整个数据集的统计特性**。  
  - 该类别序列对所有其他序列上的信息做汇聚，能更好地**关注到全局特征**，并且由于它本身不基于图像内容，因此可以避免对某个特定的输入序列**偏向性**。  
  - 对该类别序列使用固定的位置编码能够避免输出受到位置编码的干扰。vit中将class embedding视为输入序列的头部而非尾部，即位置为0。  
- 而类别令牌也可以换成**全局平均池化**，对于图像分类任务的输出结果和使用类别令牌并没有很大的差别。  
  
### 3.5 vit网络中其他相关知识

#### 3.5.1 transformer中的encoder使用

- 相比传统的使用resnet进行图像分类，vit直接用transformer encoder替换掉cnn网络，在pytorch中亦可直接通过nn.TransformerEncoderLayer来实现。在这之中包含了最为主要的多头自注意力模块和 MLP 多层线性层模块。
- 多头自注意力机制在2.2中有详细解释过，能对输入向量的全局特征进行更好的感知。而MLP实现了将输入向量的维度放大后再缩小回去，每一个block之后维度依然和输入时相同，因此可以和多头自注意力模块一起组成一个block，进行多次的堆叠，也使得网络可以加入残差连接的设计，融入resnet网络的优点，且更容易进行训练。encoder最后会将得到的结果中的class token作为最终结果输出，用这个向量作为网络分类头使用的特征信息。

#### 3.5.2 Gelu 相比起 Relu 的使用

- 不同于进行阈值硬判决的 relu 函数，relu 函数根据输入的大小赋予其应有的权重，而实现是通过最常用的高斯分布累积函数。
- relu 在当前 NLP 领域中表现最佳，尤其在 transformer 架构中表现最好。
- 能**避免梯度消失**问题。
 
#### 3.5.3 layer norm 相比起 batch norm 的使用

- Norm 的作用是它平滑了Loss，保持了**梯度下降过程中的稳定**。
- LN的使用起源于 NLP 领域，且当前NLP中主流就是用LN，实验结果表明效果更优
- BN 在Transformer中不太好用的原因可能是：Transformer原本处理的是NLP的文本序列，本质上可以看成一个**时间序列**，而时间序列是不定长的，长度不同的序列原则上属于不同的统计对象，所以**很难得到稳定的统计量**，而得不到稳定的统计量，BN就无法成立，因为BN依靠滑动平均来获得一组预测用的统计量。
  
#### 3.5.4 多头相比起单头自注意力的作用

- 多头保证了transformer可以注意到**不同子空间**的信息，捕捉到更加丰富的特征信息。其实本质上是论文原作者发现这样效果确实好。
- 多头可以使参数矩阵形成多个子空间，矩阵整体的size不变，只是改变了每个head对应的维度大小，这样做使矩阵对多方面信息进行学习，但是计算量和单个head差不多。
- 类似于cnn中多个卷积核的作用，使用多头注意力，能够从不同角度提取信息，**提高信息提取的全面性**。

### 3.5.5 传统CNN（resnet）和 CNN + Transformer 相比 ViT

-  CNN具有transformer所没有的归纳偏置的特性，且transformer又具有很强全局归纳建模能力，则使用CNN+transformer的混合模型能否得到更好的效果？这个问题在后续好像是被很多人研究过，包括串并联等各种拼接的方法，在这里不详细说明了，仅对比vit原论文中所提及的实验结果。
- 在原论文所展示的实验结果中，CNN+transformer混合模型在模型较小时的表现略优于vit网络，但在模型足够大后，混合模型的表现就没有vit网络好了。
- 由于传统的CNN网络有归纳偏置，在较小的数据集上表现更优。
- 总结来说，vit网络需要在足够大的数据集上训练表现才更优于其他网络，但vit预训练比CNN要更便宜，在相同的预训练计算复杂度下，vit的效果要比CNN更好。

## 4. DETR网络的相关知识

### 4.1 核心内容

- Detection Transformer 是一种全新的端到端的目标检测范式，图片通过CNN提取特征，然后将提取的特征展平输入transformer encoder-decoder，然后通过一系列查询，检测头输出每个查询的结果。查询的数量通常为100、300或900，远远少于之前的检测算法中的密集预测。**整个算法的流程是：Backbone&Neck ( CNN，通常是resnet )  -> 位置编码 -> Encoder -> Decoder -> FFN -> 匹配机制和loss计算**。
- DETR的思路和传统的目标检测的本质思路有相似之处，但表现方式很不一样。传统的方法比如Anchor-based方法本质上是对预定义的密集anchors进行类别的分类和边框系数的回归。DETR则是将目标检测视为一个集合预测问题（集合和anchors的作用类似）。由于Transformer本质上是一个序列转换的作用，**因此，可以将DETR视为一个从图像序列到一个集合序列的转换过程**。该集合实际上就是一个可学习的位置编码（object query, 代码中叫作query_embed）。
<img src=https://picx.zhimg.com/70/v2-b37be5d54810e8ead4364b13da53f440  width=700 height=180/> 

### 4.2 主体网络

#### 4.2.1 位置编码以及特征图转换

- DETR使用传统的CNN主干网络来学习输入图像的2D表示。之后对于输入图像转换为序列的处理，与vit网络有所不同。vit网络将原图像打散成多个图像块patch，再对每个图像块进行展平操作变为序列。**而DETR使用CNN的大卷积核和步长使原图其变成一个个小的特征图，并行展开为序列后输入encoder**，并在将其传递到转换器编码器之前用位置编码对其进行补充。
- 而DETR的位置编码与传统transformer的位置编码类似，可以用正弦余弦函数的固定位置编码，或者使用可学习的位置编码，最后将位置编码向量和输入向量相加，送入transformer块中。
- 下方为二维可学习位置编码嵌入以及object query向量的设定代码示例，与原论文中的正弦位置编码不同：
	```python
	self.query_pos = nn.Parameter(torch.rand(100, batch_size, hidden_dim)) 
	self.row_embed = nn.Parameter(torch.rand(16, hidden_dim // 2)) 
	self.column_embed = nn.Parameter(torch.rand(16, hidden_dim // 2))
	# 若位置编码过大则很容易炸显存
	```

#### 4.2.2 encoder-decoder网络结构

- DETR的encoder-decoder结构与传统transformer很类似，在这次任务中pytorch的代码就直接用nn.Transformer来代替了，（DETR官方库中对Transformer类进行了魔改，就如下图中展示的那样，难以效仿）可以实现基本相同的功能，但是具体来说仍有所欠缺。
- Encoder由6个Encoder_layer串联组成，每个Encoder_layer都会加上位置编码，这点不同于NLP只在最开始加一次位置编码。**多头注意力中各个头的线性层的参数是共享的**，Encoder输出shape与输入相同。
- Decoder由6个Decoder_layer串联组成，Decoder_layer由Self-attention和Cross-attention组成，Self-attention的输入需要query和query_pos，query初始设为零向量，会在Decoder_layer逐层更新，**每个query用于一个目标的位置和类别的预测**。
- query_pos是query的位置嵌入，在所有Decoder_layer是一样的，用nn.Embedding设置可学习的向量及个数。由于transformer的decoder端也有self-attention，因此各个位置之间可以互相通信和协作。object queries代替了传统检测中anchor的作用。即这里用query做attention。
- 6个Decoder_layer的输出都会过一个共享的FFN得到预测类别和位置结果，然后参与loss的计算，这是深监督。采用匈牙利匹配进行label Asign，定位损失为L1 loss和GIoU loss，分类损失为交叉熵损失。
<img src=https://www.kppkkp.top/usr/uploads/2023/04/2264277218.png  width=600 height=600/>  

### 4.3 损失函数

#### 4.3.1 二分图匹配

- DETR的损失函数计算分为两步，第一步是先使用**匈牙利匹配算法进行二分图匹配**，第二步是使用第一步筛选出的结果与标签进行损失函数值的计算。
- DETR在单次通过解码器时推断一个固定大小的有 N 个预测的集合，N即为前面提到的object query的值，它被设置为显著大于图像中典型的物体数量，通常为100。训练的主要困难之一是在 ground truth 方面对预测对象(类别、位置、大小)进行打分。我们的损失在预测对象和真实对象之间产生一个最佳的二分匹配，然后优化 object-speciﬁc ( bounding box ) 的损失。
- 二分图匹配实现的功能就是从N个预测结果中，依据每个标签值，挑选出一一对应匹配的预测结果组成新的集合，其他的预测结果则与no object匹配，并使得后续步骤以此来和标签计算损失函数值。理论上来说，经过这一步的操作后，**每一个object query都会有唯一匹配的目标，不会存在重叠，所以DETR不需要nms非极大抑制来进行后处理。** 匈牙利算法涉及到iou等多种指标的评判，算法过程比较复杂，这里就不展开说明了。
- 以下为matcher的部分基本代码示例：
	```python
	def match(outputs, targets):  
	    bs, num_queries = outputs["pred_logits"].shape[:2]  # 2, 10  
	 # 把每个batch中所有图像的所有预测目标合并  
	  out_prob = outputs["pred_logits"].flatten(0, 1).softmax(-1)  # [batchsize x 50预测目标数, 20]  
	  out_bbox = outputs["pred_boxes"].flatten(0, 1)  # [batch_size x 50预测目标数, 4]  
	 # 把每个batch中所有图像的所有标签合并  
	  tgt_ids = torch.cat([v["cls"] for v in targets]).to(device)  # [总目标数]  
	  tgt_bbox = torch.cat([v["box"] for v in targets]).to(device)  # [总目标数, 4]  
	  
	  cost_class = -out_prob[:, tgt_ids]  
	    cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)  
	    cost_giou = - (generalized_box_iou(out_bbox, tgt_bbox)[0])  
	    C = cost_bbox * cost_bbox + cost_class * cost_class + cost_giou * cost_giou  
	    C = C.view(bs, num_queries, -1).cpu()  # [batchsize, 预测目标数10, 总目标数]  
	  sizes = [len(v["box"]) for v in targets]  
	  
	    indices = [linear_sum_assignment(c[i].detach()) for i, c in enumerate(C.split(sizes, -1))]  
	    temp = [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]  
	    return temp
	```

#### 4.3.2 具体计算

- **分类损失：** loss_labels采用的是**交叉熵损失**，**针对所有预测结果**，包括匹配成功的和匹配失败的。匹配成功的预测结果会被放入标签对应的位置，而匹配失败的会被用num_classes来填充，num_classes代表着类别数，也代表最后一类no object空类。以下为loss_labels损失计算的部分代码示例：
	```python
	def loss_labels(self, outputs, targets, indices, num_boxes):  
	    src_logits = outputs['pred_logits']  
	    idx = self._get_src_permutation_idx(indices)  
	    target_classes_o = torch.cat([t["cls"][J] for t, (_, J) in zip(targets, indices)])  
	    target_classes = torch.full(src_logits.shape[:2], self.num_classes, dtype=torch.int64)  
	    target_classes[idx] = target_classes_o  
	    src_logits = src_logits.to("cpu")    
	  loss_ce = nn.functional.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)  
	    losses = {'loss_ce': loss_ce}  
	    return losses
	  ```
- **目标框损失：** loss_boxes包括loss_bbox和loss_giou，两者虽然都是在同一个函数中计算得到的结果，但是最后计算总损失和loss_labels一起加权的时候仍是独立计算的，可以使用不同的权重。且与loss_labels不用，loss_boxes **针对的是所有匹配成功**的预测结果。loss_bbox采用的是**L1 loss**，loss_giou代表的是generalized_box_iou损失，计算时会再次使用到matcher中的giou计算函数。以下为loss_boxes损失计算的部分代码示例：
	```python
	def loss_boxes(self, outputs, targets, indices, num_boxes):  
	    idx = self._get_src_permutation_idx(indices)  
	    src_boxes = outputs['pred_boxes'][idx]  
	    target_boxes = torch.cat([t['box'][i] for t, (_, i) in zip(targets, indices)], dim=0)  
	    src_boxes = src_boxes.to("cpu")  
	    loss_bbox = nn.functional.l1_loss(src_boxes, target_boxes, reduction='none')  
	    losses = {}  
	    temp = generalized_box_iou(src_boxes, target_boxes)  
	    losses['loss_bbox'] = loss_bbox.sum() / num_boxes  
	    loss_giou = 1 - torch.diag(temp[0])  
	    losses['loss_giou'] = loss_giou.sum() / num_boxes  
	  
	    iou = torch.diag(temp[1])  
	    iiou = iou.sum() / num_boxes  # 总iou除以框数得到平均的iou  
	  return losses, iiou
	  ```
- 总loss的计算形如（原论文中损失的计算更为复杂，需要算入每次经过transformer的模块时的损失一并加权，这里就省略了）：
	```python
	Loss = losses["loss_ce"] * 1 + losses["loss_bbox"] * 5 + losses["loss_giou"] * 2
	```

### 4.4 总结

- DETR的主要组成部分是基于集合的全局损失函数，该损失函数通过二分匹配和transformer的encoder-decoder体系结构**强制进行唯一的预测**。给定一个固定的学习对象查询的小集合，DETR会考虑目标对象与全局图像上下文之间的关系，并直接并行输出最终的预测集合。
- 一般来说DETR的效果相比起传统的目标检测网络是很不错的，在大目标检测上性能是最好的；但是小目标上稍差，而且基于match的loss导致学习很难收敛，难以学习到最优情况。DETR新框架的出现对这两个问题进行了比较好的改进。
- DETR本身较难训练，且需要在大数据集上才能表现得较为出色。在不使用预训练模型，直接从头开始训练的情况下，**object query是随机初始化的，导致很难找到图片上某一特定区域的物体**，在小数据集中训练起来则更为困难。
