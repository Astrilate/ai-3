# 人工智能cv第三轮学习笔记

## 1. 目标检测的相关知识

### 1.1 核心内容

- 目标检测是在图像或视频中识别和定位多个物体实例的任务。
- 每个检测结果通常包括物体的类别标签和位置信息，用矩形边界框表示。

### 1.2 目标检测的评估指标

- Intersection over Union (IoU)：IoU是最常用的目标检测指标之一，它衡量了模型检测框与真实框之间的重叠程度。通常，IoU阈值（如0.5）用于确定检测是否正确。
- mAP (mean Average Precision)：mAP是所有类别的AP的平均值，用于综合评估模型性能。
	- Average Precision (AP): AP是另一个常用的指标， 它测量模型在不同IoU阈值下的精度。通过计算不同IoU阈值下的Precision-Recall曲线，再计算曲线下的面积
	- PR曲线: Precision-Recall曲线
	- Precision: TP / (TP + FP)
	- Recall: TP / (TP + FN)
	- TP: IoU>0.5的检测框数量
	- FP: IoU<=0.5的检测框，或者多余检测框的数量
- 一般来说mAP针对整个数据集而言的；AP针对数据集中某一个类别而言的；而percision和recall针对单张图片某一类别的。

### 1.3 常见的目标检测方法

#### 1.3.1 两阶段检测器（如R-CNN系列）

主要思路是先通过启发式方法（selective search）或者CNN网络产生一系列稀疏的候选框，然后对这些候选框进行分类与回归。
- R-CNN (Region-based Convolutional Neural Network)：通过生成候选区域，然后使用CNN对这些区域进行分类和边界框回归，但是速度相对较慢。
- Faster R-CNN：Faster R-CNN引入了区域提议网络（RPN），来生成候选区域。它同样是CNN来提取特征并进行分类和回归。不过Faster R-CNN在准确性和速度上取得了平衡。

#### 1.3.2 单阶段检测器

主要思路是均匀地在图片的不同位置进行密集抽样，抽样时可以采用不同尺度和长宽比，然后利用CNN提取特征后直接进行分类与回归。
- YOLO (You Only Look Once)： YOLO是一种实时目标检测方法，通过单个前向传播来进行检测。它将图像分成网格，每个网格预测多个边界框和类别。YOLO最为突出的特点是速度快，准确率就比较一般。
- SSD (Single Shot MultiBox Detector) ：SSD和YOLO类似，都是用CNN网络来进行目标检测，也都采用了多尺度特征图的检测策略：通过在不同分辨率的图像上检测，来更有效地捕捉不同大小的目标。而SSD采用卷积直接做检测，Yolo在全连接层之后做检测，这里有一些不同。但是SSD相比于YOLO对于小目标的检测更准确，而速度就相比YOLO慢。

## 2. Transformer机制的相关知识

### 2.1 核心内容

- Transformer最初被提出用于自然语言处理（NLP）任务，但后来被成功地应用于计算机视觉和其他领域。它的关键是自注意力机制，它允许模型在不同位置的输入之间建立关联。在计算机视觉领域中也产生了很多变体，如VIT，DETR，swin transformer等。
- 在各种各样的任务中，注意力机制已经成为各种引人注目的序列模型和转换模型中的不可或缺的组成部分，它允许对依赖关系建模，而不需要考虑它们在输入或输出序列中的距离。 它只基于单独的attention机制，完全避免使用循环和卷积，且在处理长序列的问题上更有优势。
<img src=https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20va2t3ZWlzaGUvaW1hZ2VzL3Jhdy9tYXN0ZXIvTUwvMjAxOS05LTI1XzIzLTI1LTE0LnBuZw width=500 height=600  />

### 2.2 自注意力机制

自注意力机制是Transformer的核心组成部分，它允许模型在输入序列的不同位置之间分配不同的权重，从而捕获全局依赖性。

#### 2.2.1 注意力权重的计算

- 自注意力机制使用三个权重矩阵：查询（Query）、键（Key）和值（Value）来计算注意力权重。
- 查询矩阵决定了我们关注哪些位置，键矩阵决定了哪些位置与查询更相关，值矩阵包含了要合并的信息。
- 注意力权重(Scaled Dot-Product Attention)计算公式为:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \cdot V
$$

#### 2.2.2 多头自注意力

单个 attention 所能注意到的信息是有限的，所以通过 multi-head attention 克服这一点。
- 为了增强模型的表达能力，通常会使用多个自注意力头。多头自注意力计算多个不同的注意力权重，然后将它们合并以获得最终表示。
- 学习若干组不同的线性映射  W（权值不共享），将 query、key、value 分别投影到  dq、dk、dv  维的空间里（等价于一维卷积），每一组称为一个 head。再对投影后的结果，并行计算 attention，得到  dv  维的值。之后拼接并经过线性映射  W0，得到最终的 multi-head attention。
- heads过多过少都会损失信息
<img src=https://sighsmile.github.io/assets/attention.png width=500 height=300 />

### 2.3 编码器和解码器

Transformer模型通常包括编码器和解码器，每个部分都由多层堆叠的自注意力层和前馈神经网络层组成。

#### 2.3.1 编码器

- 编码器用于处理输入序列，例如图像的像素或自然语言句子。它将输入序列转换为高维表示。
- 编码器由N=6个相同层的堆叠组成。每一层都有两个子层。第一种是多头自注意机制，第二种是简单的位置全连接前馈网络。在两个子层的每一个周围采用残差连接，随后是层标准化。

#### 2.3.2 解码器

- 解码器利用高维的信息，生成输出序列，如机器翻译的目标语言句子。
- 解码器也由N=6个相同层的堆栈组成。除了每个编码层中的两个子层之外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意。与编码器类似，在每个子层周围使用残差连接，然后进行层标准化。它还包括额外的遮蔽（mask）层，以防止关注到后续位置(利用掩码以防止位置参与后续的位置运算)。这种掩码与输出嵌入偏移一个位置的事实相结合，确保了对位置i的预测只能依赖于小于i的位置处的已知输出。

### 2.4 位置编码

- 由于Transformer没有明确的位置信息，位置编码被引入来提供输入序列中各个位置的位置信息。位置编码与嵌入具有相同的维度dmodel，因此这两者可以相加。
- 常见的位置编码方法包括基于正弦和余弦函数的编码。通过正弦和余弦函数的组合，以一种逐渐减小的频率为不同位置和维度计算位置编码值。这样，每个位置和维度的位置编码都是唯一的，以便模型能够识别不同位置的标记。公式如下：

$$
PE_{\left(\text{pos},2i\right)} = \sin\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right) ,
PE_{\left(\text{pos},2i+1\right)} = \cos\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)
$$

### 2.5 总结

- Transformer模型在处理长距离依赖性和全局关联性方面表现出色。
- 它易于并行化，使其在大规模数据上训练时具有优势。
- Transformer需要大量的参数和计算资源，且训练时间长，因此在较小的数据集上可能不适用。 
- 对于一维序列数据，如音频信号，传统的RNN和CNN仍然有优势。

## 3. ViT网络的相关知识

### 3.1 核心内容

- Transformer模型在NLP领域研究中显示出主导趋势，这是因为他们有很强的能力通过自注意机制来模拟长期依赖。而ViT主要的思想是将NLP中的transformer机制引入到图像处理中。
- 在其原论文中提出的核心结论是：当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差，因为Transformer和CNN相比缺少归纳偏置（inductive bias），即一种先验知识，提前做好的假设。
<img src=https://zyc.ai/transformer/vision_transformer/model_scheme.svg  width=600 height=300/>

### 3.2 图像块的划分patch embedding

- Transformer的输入应为序列数据，格式为[batchsize, sentence_len, dim], 而最常用的图像数据格式为[batchsize, channels, height, width], 因此，要将Transformer应用于计算机视觉领域，首先得进行数据转换，即将图像数据转换为序列数据。
- 例如3x224x224的图像，若将图像分为固定大小16x16的patch，则每张图就有14x14+1=197个这样的patch(额外的一个是特殊字符class token，在每个序列的开头)，每个patch维度是3x16x16=768，最后输入张量的形状则为[batchsize, 197, 768]。

### 3.3 位置编码的嵌入positional embedding

- 位置编码可以理解为一张表，表一共有N行，N的大小和输入序列长度相同（197），每一行代表一个向量，向量的维度和输入序列的维度相同（768）。通过sum加入位置编码信息之后，输入张量的维度依然是197x768。
- 部分vit网络的位置嵌入沿用传统transformer的位置嵌入方式，使用正弦余弦函数的方式进行嵌入，而也有部分vit网络使用可学习的位置编码，把参数添加到可训练的参数中一起训练优化，如下方代码。
```python
self.positions = nn.Parameter(torch.randn((image_size // patch_size) **2 + 1, embedding_size))
```
- 位置编码包括一维和二维的编码方式，一维的编码方式为把每个patch按顺序从0到197编码，二维的编码方式为同时考虑x轴和y轴的信息，类似11,12这种，而vit网络使用的是一维的编码方式，实际上实验表明一维和二维的编码方式对于结果没有太大的区别，至于原因可能是因为位置编码作用在图像块上而不是像素上，图像块之间的位置关系很容易理解。

### 3.4 类别令牌的使用（class token）

- 在3.3的例子中，假设已经把输入图像分割为14x14个16x16x3的图像块后，输入序列为196，由于vit使用transformer的encoder部分作为主体网络，而196的序列的输出也为196个编码向量，最终的分类头需要使用一个向量。所以vit网络就引入了class token作为类别令牌，和196个输入序列一起输入transformer网络中，而最终分类头进行分类的时候只需要对这一个类别令牌的向量进行分类预测即可。
- vit使用上述的类别令牌进行分类预测，原因可能包括以下几点:
  - 类别序列随机初始化，并随着网络的训练不断更新，它能够编码整个数据集的统计特性。
  - 该类别序列对所有其他序列上的信息做汇聚，能更好地关注到全局特征，并且由于它本身不基于图像内容，因此可以避免对某个特定的输入序列偏向性。
  - 对该类别序列使用固定的位置编码能够避免输出受到位置编码的干扰。vit中将class embedding视为输入序列的头部而非尾部，即位置为0。
- 而类别令牌也可以换成全局平均池化，对于图像分类任务的输出结果和使用类别令牌并没有很大的差别。

### 3.5 vit网络中的主体结构

#### 3.5.1 transformer中的encoder使用
- vit的主体网络结构大部分直接沿用了transformer网络的结构，
#### 3.5.2 MLP  
#### 3.5.3 layer norm的使用
## 4. DETR网络的相关知识
