# 人工智能cv第三轮学习笔记

## 1. 目标检测的相关知识

### 1.1 核心内容

- 目标检测是在图像或视频中识别和定位多个物体实例的任务。
- 每个检测结果通常包括物体的类别标签和位置信息，用矩形边界框表示。

### 1.2 目标检测的评估指标

- Intersection over Union (IoU)：IoU是最常用的目标检测指标之一，它衡量了模型检测框与真实框之间的重叠程度。通常，IoU阈值（如0.5）用于确定检测是否正确。
- Average Precision (AP)：AP是另一个常用的指标， 它测量模型在不同IoU阈值下的精度。通过计算不同IoU阈值下的Precision-Recall曲线，再计算曲线下的面积，即AP。
- mAP (mean Average Precision)：mAP是所有类别的AP的平均值，用于综合评估模型性能。
___
### 1.3 常见的目标检测方法

#### 1.3.1 两阶段检测器（如R-CNN系列）

主要思路是先通过启发式方法（selective search）或者CNN网络产生一系列稀疏的候选框，然后对这些候选框进行分类与回归。
- R-CNN (Region-based Convolutional Neural Network)：通过生成候选区域，然后使用CNN对这些区域进行分类和边界框回归，但是速度相对较慢。
- Faster R-CNN：Faster R-CNN引入了区域提议网络（RPN），来生成候选区域。它同样是CNN来提取特征并进行分类和回归。不过Faster R-CNN在准确性和速度上取得了平衡。
___
#### 1.3.2 单阶段检测器

主要思路是均匀地在图片的不同位置进行密集抽样，抽样时可以采用不同尺度和长宽比，然后利用CNN提取特征后直接进行分类与回归。
- YOLO (You Only Look Once)： YOLO是一种实时目标检测方法，通过单个前向传播来进行检测。它将图像分成网格，每个网格预测多个边界框和类别。YOLO最为突出的特点是速度快，准确率就比较一般。
- SSD (Single Shot MultiBox Detector) ：SSD和YOLO类似，都是用CNN网络来进行目标检测，也都采用了多尺度特征图的检测策略：通过在不同分辨率的图像上检测，来更有效地捕捉不同大小的目标。而SSD采用卷积直接做检测，Yolo在全连接层之后做检测，这里有一些不同。但是SSD相比于YOLO对于小目标的检测更准确，而速度就相比YOLO慢。

## 2. Transformer机制的相关知识

### 2.1 核心内容

- Transformer最初被提出用于自然语言处理（NLP）任务，但后来被成功地应用于计算机视觉和其他领域。它的关键是自注意力机制，它允许模型在不同位置的输入之间建立关联。在计算机视觉领域中也产生了很多变体，如VIT，DETR，swin transformer等。
![tranformer架构](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20va2t3ZWlzaGUvaW1hZ2VzL3Jhdy9tYXN0ZXIvTUwvMjAxOS05LTI1XzIzLTI1LTE0LnBuZw?x-oss-process=image/format,png)
- 在各种各样的任务中，注意力机制已经成为各种引人注目的序列模型和转换模型中的不可或缺的组成部分，它允许对依赖关系建模，而不需要考虑它们在输入或输出序列中的距离。 它只基于单独的attention机制，完全避免使用循环和卷积，且在处理长序列的问题上更有优势。
___
### 2.2 自注意力机制

自注意力机制是Transformer的核心组成部分，它允许模型在输入序列的不同位置之间分配不同的权重，从而捕获全局依赖性。

#### 2.2.1 注意力权重的计算

- 自注意力机制使用三个权重矩阵：查询（Query）、键（Key）和值（Value）来计算注意力权重。
- 查询矩阵决定了我们关注哪些位置，键矩阵决定了哪些位置与查询更相关，值矩阵包含了要合并的信息。
- 注意力权重(Scaled Dot-Product Attention)计算公式为：$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \cdot V$$

#### 2.2.2 多头自注意力

单个 attention 所能注意到的信息是有限的，所以通过 multi-head attention 克服这一点
- 为了增强模型的表达能力，通常会使用多个自注意力头。多头自注意力计算多个不同的注意力权重，然后将它们合并以获得最终表示。
- 学习若干组不同的线性映射  W（权值不共享），将 query、key、value 分别投影到  dq、dk、dv  维的空间里（等价于一维卷积），每一组称为一个 head。再对投影后的结果，并行计算 attention，得到  dv  维的值。之后拼接并经过线性映射  W0，得到最终的 multi-head attention。
- heads过多过少都会损失信息

![tranformer架构](https://sighsmile.github.io/assets/attention.png)
___
### 2.3 编码器和解码器

Transformer模型通常包括编码器和解码器，每个部分都由多层堆叠的自注意力层和前馈神经网络层组成。

#### 2.3.1 编码器

- 编码器用于处理输入序列，例如图像的像素或自然语言句子。它将输入序列转换为高维表示。
- 编码器由N=6个相同层的堆叠组成。每一层都有两个子层。第一种是多头自注意机制，第二种是简单的位置全连接前馈网络。在两个子层的每一个周围采用残差连接，随后是层标准化。
 
#### 2.3.2 解码器

- 解码器利用高维的信息，生成输出序列，如机器翻译的目标语言句子。
- 解码器也由N=6个相同层的堆栈组成。除了每个编码层中的两个子层之外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意。与编码器类似，在每个子层周围使用残差连接，然后进行层标准化。它还包括额外的遮蔽（mask）层，以防止关注到后续位置(利用掩码以防止位置参与后续的位置运算)。这种掩码与输出嵌入偏移一个位置的事实相结合，确保了对位置i的预测只能依赖于小于i的位置处的已知输出。
___
### 2.4 位置编码

- 由于Transformer没有明确的位置信息，位置编码被引入来提供输入序列中各个位置的位置信息。位置编码与嵌入具有相同的维度dmodel，因此这两者可以相加。
- 常见的位置编码方法包括基于正弦和余弦函数的编码。通过正弦和余弦函数的组合，以一种逐渐减小的频率为不同位置和维度计算位置编码值。这样，每个位置和维度的位置编码都是唯一的，以便模型能够识别不同位置的标记。公式如下：
$$
PE_{\left(\text{pos},2i\right)} = \sin\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right) , 
PE_{\left(\text{pos},2i+1\right)} = \cos\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)
$$

### 2.5 总结

- Transformer模型在处理长距离依赖性和全局关联性方面表现出色。
- 它易于并行化，使其在大规模数据上训练时具有优势。
- Transformer需要大量的参数和计算资源，且训练时间长，因此在较小的数据集上可能不适用。
- 对于一维序列数据，如音频信号，传统的RNN和CNN仍然有优势。

## 3. ViT网络的相关知识






